{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "57889fa3",
      "metadata": {},
      "source": [
        "# Appendix — Diagnostics, Ablations, and Learning Curves\n",
        "\n",
        "Technical deep-dive supporting the main notebook: extended diagnostics, ablations, coefficient checks, and learning curves."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e58cd8e5",
      "metadata": {},
      "source": [
        "## How to use this appendix\n",
        "\n",
        "This appendix supports the main results by stress‑testing assumptions and stability.\n",
        "\n",
        "- Residual Diagnostics: check linearity and constant variance in residuals.\n",
        "- Test Error Diagnostics: ECDF, top absolute errors, and signed error vs predicted (small n → illustrative only).\n",
        "- Ablations (OLS subsets): compare feature subsets; flag those within `DECISION_MARGIN` and prefer simpler when tied.\n",
        "- Learning Curves: when available, show MAE/uncertainty vs sample size; if skipped, more data is needed.\n",
        "- Coefficient Checks: sanity‑check signs/magnitudes and verify manual reconstruction.\n",
        "- Reproducibility: library versions for reference."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e926b53",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28c4dc7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from IPython.display import display, Markdown\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from utils import (\n",
        "    SEED as SEED_DEFAULT, N_GRID_DEFAULT, MAPE_THRESHOLD_DEFAULT, DECISION_MARGIN_DEFAULT,\n",
        "    CV_SPLITS_A_DEFAULT, CV_SPLITS_DEFAULT, CV_REPEATS_DEFAULT, BOOT_B_DEFAULT, FEATURES,\n",
        "    load_records, validate_records,\n",
        "    approach_b_ols, approach_b_ridge_cv,\n",
        "    plot_residuals_vs, ols_ablations,\n",
        "    learning_curve, plot_learning_curve,\n",
        "    coef_sanity_checks,\n",
        "    cv_single_feature, select_best,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e950975e",
      "metadata": {},
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e45408",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overrides\n",
        "SEED = SEED_DEFAULT\n",
        "N_GRID = N_GRID_DEFAULT\n",
        "DECISION_MARGIN = DECISION_MARGIN_DEFAULT\n",
        "CV_SPLITS = CV_SPLITS_DEFAULT\n",
        "CV_REPEATS = CV_REPEATS_DEFAULT\n",
        "BOOT_B = BOOT_B_DEFAULT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f1485e",
      "metadata": {},
      "source": [
        "### Load Data and Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d5e276",
      "metadata": {},
      "outputs": [],
      "source": [
        "base = Path('..').resolve()\n",
        "records_path = base / 'output' / 'records.jsonl'\n",
        "print({'records_path': str(records_path)})\n",
        "\n",
        "df = load_records(records_path)\n",
        "df = validate_records(df)\n",
        "print(df.attrs.get('validation_info', {}))\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b32d17a",
      "metadata": {},
      "source": [
        "## Residual Diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f52eeb1",
      "metadata": {},
      "source": [
        "Fit OLS on train/test and plot residual diagnostics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbb8c64f",
      "metadata": {},
      "outputs": [],
      "source": [
        "res_b_ols = approach_b_ols(train_df, test_df)\n",
        "\n",
        "yhat = res_b_ols['pred']\n",
        "resid = res_b_ols['yte'] - yhat\n",
        "\n",
        "axes = plot_residuals_vs(yhat, resid, x_series=test_df['bytes'], x_label='bytes')\n",
        "fig = axes[0].get_figure() if isinstance(axes, np.ndarray) else axes.get_figure()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "396e644b",
      "metadata": {},
      "source": [
        "### Takeaways\n",
        "\n",
        "- No clear residual pattern on this split; one moderate outlier.\n",
        "- Sample size on test is small → treat linearity/variance conclusions as tentative."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a434c27",
      "metadata": {},
      "source": [
        "## Test Error Diagnostics\n",
        "\n",
        "Detailed test error views supporting the main notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b1c767",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the OLS best subset predictions computed earlier in this appendix\n",
        "y_true = res_b_ols['yte']\n",
        "y_pred = res_b_ols['pred']\n",
        "\n",
        "abs_err = np.abs(y_true - y_pred)\n",
        "med = float(np.median(abs_err))\n",
        "p95 = float(np.percentile(abs_err, 95))\n",
        "mx  = float(np.max(abs_err))\n",
        "n   = len(abs_err)\n",
        "\n",
        "err_df = pd.DataFrame({'abs_err': np.sort(abs_err)})\n",
        "display(Markdown(\"#### Test absolute errors (sorted)\"))\n",
        "display(err_df.T)\n",
        "\n",
        "# ECDF\n",
        "fig, ax = plt.subplots(figsize=(5, 3.2))\n",
        "xs = np.sort(abs_err)\n",
        "ys = np.arange(1, n + 1) / n\n",
        "ax.step(xs, ys, where='post', color='steelblue')\n",
        "ax.axhline(0.95, ls='--', color='gray', lw=1)\n",
        "ax.axvline(p95, ls='--', color='gray', lw=1)\n",
        "ax.set_title('ECDF of absolute error')\n",
        "ax.set_xlabel('absolute error (tokens)')\n",
        "ax.set_ylabel('F(x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Signed error diagnostics (table + scatter)\n",
        "signed_err = y_true - y_pred\n",
        "tmp = pd.DataFrame({\n",
        "    'y_true': y_true,\n",
        "    'y_pred': y_pred,\n",
        "    'signed_err': signed_err,\n",
        "    'abs_err': abs_err\n",
        "}).sort_values('abs_err', ascending=False).reset_index(drop=True)\n",
        "display(Markdown(\"#### Test predictions and errors (sorted by abs_err)\"))\n",
        "display(tmp)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5,3))\n",
        "ax.scatter(y_pred, signed_err, color='steelblue', alpha=0.9)\n",
        "ax.axhline(0, color='gray', lw=1)\n",
        "ax.set_xlabel('predicted tokens')\n",
        "ax.set_ylabel('signed error (y_true - y_pred)')\n",
        "ax.set_title('Signed error vs predicted')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93ad25ee",
      "metadata": {},
      "source": [
        "### Takeaways\n",
        "- n=5 test set → indicative only.\n",
        "- On this split, median and max absolute errors provide a quick sense check; ECDF/table are included for transparency.\n",
        "- We rely on train‑side repeated‑CV with bootstrap CIs for stability; collect more test samples before drawing distributional conclusions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc5db9ea",
      "metadata": {},
      "source": [
        "## Ablations (OLS Subsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c118131",
      "metadata": {},
      "source": [
        "Feature configurations are marked as `within_margin` when their MAE is within `DECISION_MARGIN` of the best MAE. Treat those as equivalently good on this dataset; prefer the simpler set unless a larger set gives a clear, robust gain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd6c6ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run OLS ablations and flag within-margin results\n",
        "features = FEATURES\n",
        "\n",
        "# All non-empty subsets (size 1..4), ordered by size then lexicographic\n",
        "CANDIDATES = [\n",
        "    list(cols)\n",
        "    for r in range(1, len(features) + 1)\n",
        "    for cols in combinations(sorted(features), r)\n",
        "]\n",
        "\n",
        "abl_results = ols_ablations(train_df, test_df, CANDIDATES)\n",
        "df_abl = pd.DataFrame(abl_results)\n",
        "if not df_abl.empty:\n",
        "    df_abl = df_abl.sort_values('mae').reset_index(drop=True)\n",
        "    best_mae = df_abl['mae'].iloc[0]\n",
        "\n",
        "    df_abl['within_margin'] = ((df_abl['mae'] - best_mae) / best_mae) <= DECISION_MARGIN\n",
        "\n",
        "    def _bold_within_margin(row):\n",
        "        return ['font-weight: bold' if bool(row['within_margin']) else '' for _ in row]\n",
        "\n",
        "    styled = df_abl.style.apply(_bold_within_margin, axis=1)\n",
        "    display(styled)\n",
        "else:\n",
        "    print({'ablations': 'no results'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "638f46b7",
      "metadata": {},
      "source": [
        "### Takeaways\n",
        "\n",
        "- Best subset on this split: bytes+words.\n",
        "- bytes+runes+words is within the decision margin (but prefer the simpler set unless a robust gain appears)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5570880b",
      "metadata": {},
      "source": [
        "## Learning Curves"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d98ef26",
      "metadata": {},
      "source": [
        "Learning curves for A (selected single feature) and B (OLS 4 features).\n",
        "\n",
        "Curves are computed only when `N_GRID` points do not exceed the available training size. When skipped on small datasets, interpret this as \"insufficient sample sizes to form a meaningful curve\"; with more data, expect MAE to decrease and variance bands to tighten as `n` grows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "927199b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "features_list = FEATURES\n",
        "rows = []\n",
        "for feat in features_list:\n",
        "    Xtr_feat = train_df[[feat]].to_numpy()\n",
        "    ytr = train_df['input_tokens'].to_numpy()\n",
        "    for fi in (False, True):\n",
        "        mae_cv, bias_cv = cv_single_feature(Xtr_feat, ytr, fit_intercept=fi, n_splits=CV_SPLITS_A_DEFAULT, seed=SEED)\n",
        "        rows.append({'feature': feat, 'fit_intercept': fi, 'cv_mae': mae_cv, 'cv_bias': bias_cv})\n",
        "cv_df = pd.DataFrame(rows).sort_values(['cv_mae', 'feature', 'fit_intercept']).reset_index(drop=True)\n",
        "best_cv = select_best(cv_df)\n",
        "feat = best_cv['feature']\n",
        "fi = bool(best_cv['fit_intercept'])\n",
        "\n",
        "# Pre-filter N_GRID\n",
        "N_GRID_filtered = [int(n) for n in N_GRID if int(n) <= len(train_df)]\n",
        "if not N_GRID_filtered:\n",
        "    print({'learning_curves': 'skipped', 'reason': 'n_grid > len(train_df)', 'len_train_df': int(len(train_df)), 'N_GRID': list(N_GRID)})\n",
        "    display(Markdown(\"Learning curves skipped (grid > train size). With more data, expect MAE to decrease and confidence bands to tighten.\"))\n",
        "else:\n",
        "    # Determine feature_B for learning curve: use best subset from ablations if available\n",
        "    if 'df_abl' in globals() and isinstance(df_abl, pd.DataFrame) and not df_abl.empty and 'cols' in df_abl.columns:\n",
        "        feature_B_cols = tuple(df_abl.loc[0, 'cols'])\n",
        "    else:\n",
        "        feature_B_cols = ('bytes','runes','words','lines')\n",
        "    print({'learning_curve_feature_B': feature_B_cols})\n",
        "    # Ensure CV splits are valid for the smallest n in the grid\n",
        "    cv_splits_lc = max(2, min(CV_SPLITS, min(N_GRID_filtered)))\n",
        "    print({'cv_splits_lc': cv_splits_lc, 'cv_repeats': CV_REPEATS})\n",
        "    lc_df = learning_curve(\n",
        "        train_df,\n",
        "        n_grid=N_GRID_filtered,\n",
        "        feature_A=(feat,),\n",
        "        feature_B=feature_B_cols,\n",
        "        seed=SEED,\n",
        "        cv_splits=cv_splits_lc,\n",
        "        cv_repeats=CV_REPEATS,\n",
        "        boot_B=BOOT_B,\n",
        "        fit_intercept_A=fi,\n",
        "    )\n",
        "    print({'learning_curve_feature_A': feat, 'fit_intercept_A': fi})\n",
        "\n",
        "    if lc_df is not None and not lc_df.empty:\n",
        "        display(lc_df)\n",
        "        ax = plot_learning_curve(lc_df)\n",
        "        ax.set_title(f'Learning curves with {cv_splits_lc}x{CV_REPEATS} CV and bootstrap CIs')\n",
        "        handles, _ = ax.get_legend_handles_labels()\n",
        "        ax.legend(handles, ['A (bytes + intercept)', f'B (OLS: {\",\".join(feature_B_cols)})'])\n",
        "    else:\n",
        "        print({'learning_curves': 'skipped_or_empty'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa2125bc",
      "metadata": {},
      "source": [
        "### Takeaways\n",
        "\n",
        "- Curves skipped due to small train size; with more data, expect MAE to decrease and bands to tighten."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb1fade",
      "metadata": {},
      "source": [
        "## Coefficient Checks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cb4afd4",
      "metadata": {},
      "source": [
        "Sanity flags on best B model and coefficient recovery demos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b14f96",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coefficient checks using OLS best-subset (from ablations if available)\n",
        "from itertools import combinations\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Determine best subset\n",
        "best_cols = None\n",
        "best_res = None\n",
        "\n",
        "# Prefer the ablations table if it exists\n",
        "if 'df_abl' in globals() and isinstance(df_abl, pd.DataFrame) and not df_abl.empty:\n",
        "    best_cols = df_abl.loc[0, 'cols'] if 'cols' in df_abl.columns else None\n",
        "\n",
        "# Fallback: recompute best subset by test MAE\n",
        "if best_cols is None:\n",
        "    features = FEATURES\n",
        "    best_mae = np.inf\n",
        "    for r in range(1, len(features) + 1):\n",
        "        for cols in combinations(sorted(features), r):\n",
        "            Xtr = train_df[list(cols)].to_numpy()\n",
        "            ytr = train_df['input_tokens'].to_numpy()\n",
        "            Xte = test_df[list(cols)].to_numpy()\n",
        "            yte = test_df['input_tokens'].to_numpy()\n",
        "            lr = LinearRegression(fit_intercept=True)\n",
        "            lr.fit(Xtr, ytr)\n",
        "            pte = lr.predict(Xte)\n",
        "            mae = float(np.mean(np.abs(yte - pte)))\n",
        "            if mae < best_mae:\n",
        "                best_mae = mae\n",
        "                best_res = {\n",
        "                    'cols': list(cols),\n",
        "                    'intercept': float(lr.intercept_),\n",
        "                    'coefs': dict(zip(list(cols), map(float, lr.coef_))),\n",
        "                    'pred': pte,\n",
        "                }\n",
        "    best_cols = best_res['cols']\n",
        "    intercept = best_res['intercept']\n",
        "    coefs = best_res['coefs']\n",
        "    pred = best_res['pred']\n",
        "else:\n",
        "    # Refit on best_cols to capture exact coefficients and predictions\n",
        "    Xtr = train_df[best_cols].to_numpy()\n",
        "    ytr = train_df['input_tokens'].to_numpy()\n",
        "    Xte = test_df[best_cols].to_numpy()\n",
        "    lr = LinearRegression(fit_intercept=True)\n",
        "    lr.fit(Xtr, ytr)\n",
        "    pred = lr.predict(Xte)\n",
        "    intercept = float(lr.intercept_)\n",
        "    coefs = dict(zip(best_cols, map(float, lr.coef_)))\n",
        "\n",
        "print({'best_subset_cols': best_cols})\n",
        "\n",
        "# Sanity flags and manual reconstruction\n",
        "flags = coef_sanity_checks(coefs)\n",
        "print({'ols_subset_flags': flags})\n",
        "\n",
        "coef_order = sorted(coefs.keys())\n",
        "Xte = test_df[coef_order].to_numpy()\n",
        "coef_vec = np.array([coefs[k] for k in coef_order], dtype=float)\n",
        "manual_pred = intercept + Xte @ coef_vec\n",
        "ok = np.allclose(manual_pred, pred, atol=1e-6)\n",
        "max_diff = float(np.max(np.abs(manual_pred - pred)))\n",
        "print({'ols_subset_coef_recovery_check': bool(ok), 'max_abs_diff': max_diff})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f149b486",
      "metadata": {},
      "source": [
        "### Takeaways\n",
        "\n",
        "- Sanity flags are clear; manual reconstruction matches predictions.\n",
        "- Linear model behavior verified on this split."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa0e1894",
      "metadata": {},
      "source": [
        "## Reproducibility\n",
        "\n",
        "Library versions for reference (see below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e20f0078",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Markdown(f\"\"\"Library versions:\\n{'\\n'.join([f'- {k}: {v}' for k, v in {'numpy': np.__version__, 'pandas': pd.__version__, 'sklearn': sklearn.__version__}.items()])}\"\"\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": " (.venv) token-approx (Python 3.13.5)",
      "language": "python",
      "name": "token-approx"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
