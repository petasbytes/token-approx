{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c32bfc89",
      "metadata": {},
      "source": [
        "# Token Approximation — EDA and Baselines\n",
        "\n",
        "Concise analysis and baseline models to approximate input token counts from local text features.\n",
        "\n",
        "Technical depth (ablations, diagnostics, learning curves) is in `02_appendix_diagnostics.ipynb`.\n",
        "\n",
        "## Overview and Hypothesis\n",
        "\n",
        "- Hypothesis: Local text features (bytes, runes, words, lines) can approximate provider language model input token counts.\n",
        "- Why it matters: Enables local budgeting without remote tokenizers, helpful for throughput, cost, and context management.\n",
        "- Approach: Benchmark a simple single-feature baseline versus multifeature linear models with regularization; report held-out test performance plus train-CV stability.\n",
        "- Scope: Token counts are tokenizer- and language model-specific; external validity may vary.\n",
        "- Terminology: “runes” = Unicode code points; features used: bytes, runes, words, lines.\n",
        "\n",
        "## Data\n",
        "\n",
        "- Data source: `data/processed/features/features.jsonl` produced by the Go features runner over sample texts.\n",
        "- Columns: `bytes`, `runes` (Unicode code points), `words`, `lines`, `input_tokens`, `model`, `source_path`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1998b8a5",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e556f908",
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from IPython.display import display, Markdown\n",
        "from sklearn.linear_model import ElasticNet, LinearRegression, Ridge\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from utils import (\n",
        "    SEED as SEED_DEFAULT, N_GRID_DEFAULT, MAPE_THRESHOLD_DEFAULT, DECISION_MARGIN_DEFAULT,\n",
        "    CV_SPLITS_A_DEFAULT, CV_SPLITS_DEFAULT, CV_REPEATS_DEFAULT, BOOT_B_DEFAULT, FEATURES,\n",
        "    load_records, validate_records, corr_heatmap,\n",
        "    cv_single_feature, select_best, fit_single_feature,\n",
        "    approach_b_ols, approach_b_ridge_cv, approach_b_elasticnet_cv,\n",
        "    summarize_and_decide, repeated_cv_mae, bootstrap_mae_ci,\n",
        "    export_single_feature, export_linear_multifeature, coef_sanity_checks,\n",
        ")\n",
        "\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.width', 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b46569",
      "metadata": {},
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1546d00b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defaults can be overridden as needed\n",
        "SEED = SEED_DEFAULT  # random seed for splits/CV\n",
        "N_GRID = N_GRID_DEFAULT  # learning-curve grid\n",
        "MAPE_THRESHOLD = MAPE_THRESHOLD_DEFAULT  # floor for computing MAPE\n",
        "DECISION_MARGIN = DECISION_MARGIN_DEFAULT  # min relative MAE gain to pick B over A\n",
        "CV_SPLITS_A = CV_SPLITS_A_DEFAULT  # folds for A-selection CV\n",
        "CV_SPLITS = CV_SPLITS_DEFAULT  # folds for stability CV + learning curves\n",
        "CV_REPEATS = CV_REPEATS_DEFAULT  # repeats for stability CV\n",
        "BOOT_B = BOOT_B_DEFAULT  # bootstrap resamples\n",
        "MODEL_NAME = \"claude-3-7-sonnet-latest\"  # tokenizer/model scope\n",
        "\n",
        "display(Markdown(\n",
        "    f\"\"\"Constants set for this run:\n",
        "- `SEED`=`{SEED}`\n",
        "- `DECISION_MARGIN`=`{DECISION_MARGIN}`\n",
        "- `MAPE_THRESHOLD`=`{MAPE_THRESHOLD}`\n",
        "- `CV_SPLITS` x `CV_REPEATS`=`{CV_SPLITS}` x `{CV_REPEATS}`\n",
        "\"\"\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a9f2894",
      "metadata": {},
      "source": [
        "### Load and Validate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56512b60",
      "metadata": {},
      "outputs": [],
      "source": [
        "base = Path('..').resolve()\n",
        "features_path = base / 'data' / 'processed' / 'features' / 'features.jsonl'\n",
        "print({'features_path': str(features_path)})\n",
        "\n",
        "df = load_records(features_path)\n",
        "df = validate_records(df)\n",
        "print(df.attrs.get('validation_info', {}))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e58cab6c",
      "metadata": {},
      "source": [
        "## Exploratory Visuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e552ed9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "# 2x2 scatter grid with trend lines\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
        "pairs = [('runes','input_tokens'), ('bytes','input_tokens'), ('words','input_tokens'), ('lines','input_tokens')]\n",
        "for ax, (x,y) in zip(axes.ravel(), pairs):\n",
        "    sns.regplot(data=df, x=x, y=y, scatter_kws={'s':20, 'alpha':0.8}, line_kws={'color':'orange'}, ax=ax)\n",
        "    ax.set_title(f'{y} vs {x}')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Correlation heatmap\n",
        "ax = corr_heatmap(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e32801b",
      "metadata": {},
      "source": [
        "### Takeaways\n",
        "\n",
        "- Strong linear trends between input tokens and bytes/runes.\n",
        "- Features are correlated; multicollinearity likely → regularization/subset selection is appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea4b8e71",
      "metadata": {},
      "source": [
        "## Approach A — Single-Feature Linear Regression: CV Selection\n",
        "\n",
        "Train: CV to select feature/intercept. Evaluate on a held‑out test split.\n",
        "\n",
        "MAE is the headline metric. MAPE is reported only when `y`>=`MAPE_THRESHOLD` to avoid instability on small targets.\n",
        "\n",
        "Single-feature baseline calibrates expectations before multi-feature models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ff070d",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
        "\n",
        "# CV across single features and intercept options\n",
        "features_list = FEATURES\n",
        "rows = []\n",
        "for feat in features_list:\n",
        "    Xtr_feat = train_df[[feat]].to_numpy()\n",
        "    ytr = train_df['input_tokens'].to_numpy()\n",
        "    for fi in (False, True):\n",
        "        mae_cv, bias_cv = cv_single_feature(Xtr_feat, ytr, fit_intercept=fi, n_splits=CV_SPLITS_A, seed=SEED)\n",
        "        rows.append({'feature': feat, 'fit_intercept': fi, 'cv_mae': mae_cv, 'cv_bias': bias_cv})\n",
        "cv_results = pd.DataFrame(rows).sort_values(['cv_mae', 'feature', 'fit_intercept']).reset_index(drop=True)\n",
        "\n",
        "best_cv = select_best(cv_results)\n",
        "res_a = fit_single_feature(train_df, test_df, feature=best_cv['feature'], fit_intercept=bool(best_cv['fit_intercept']))\n",
        "\n",
        "display(Markdown('#### CV results (train) (Approach A)'))\n",
        "display(cv_results)\n",
        "\n",
        "y_true_sf = res_a['yte']; y_pred_sf = res_a['pred']\n",
        "mask = y_true_sf >= MAPE_THRESHOLD\n",
        "mape = None if not mask.any() else float(np.mean(np.abs((y_true_sf[mask] - y_pred_sf[mask]) / y_true_sf[mask])))\n",
        "\n",
        "display(Markdown('#### Held-out Test Results (Approach A)'))\n",
        "display(pd.DataFrame([{\n",
        "    'stage': 'A_single_feature_test',\n",
        "    'feature': res_a['feature'],\n",
        "    'fit_intercept': res_a['fit_intercept'],\n",
        "    'test_mae': res_a['test_mae'],\n",
        "    'test_bias': res_a['test_bias'],\n",
        "    'MAPE_after_filter': mape,\n",
        "    'MAPE_after_filter_pct': None if mape is None else 100*mape,\n",
        "}]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d06619ef",
      "metadata": {},
      "source": [
        "## Approach B — Multi-Feature Linear Regression: OLS-best-subset; Ridge/ElasticNet via CV\n",
        "\n",
        "Train: OLS subset search; Ridge/ElasticNet via CV. Evaluate on a held‑out test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae7a502f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Regularized baselines (Ridge, ElasticNet)\n",
        "res_b_ridge = approach_b_ridge_cv(train_df, test_df, seed=SEED)\n",
        "res_b_enet = approach_b_elasticnet_cv(train_df, test_df, seed=SEED)\n",
        "\n",
        "# 2) OLS best-subset on train/test (exhaustive over all non-empty feature subsets)\n",
        "def fit_ols_subset_return_full(train_df, test_df, cols):\n",
        "    Xtr = train_df[list(cols)].to_numpy()\n",
        "    ytr = train_df['input_tokens'].to_numpy()\n",
        "    Xte = test_df[list(cols)].to_numpy()\n",
        "    yte = test_df['input_tokens'].to_numpy()\n",
        "\n",
        "    lr = LinearRegression(fit_intercept=True)\n",
        "    lr.fit(Xtr, ytr)\n",
        "    pte = lr.predict(Xte)\n",
        "    mae = float(mean_absolute_error(yte, pte))\n",
        "    bias = float(np.mean(yte - pte))\n",
        "    r2 = float(r2_score(yte, pte))\n",
        "    coefs = dict(zip(list(cols), map(float, lr.coef_)))\n",
        "\n",
        "    return {\n",
        "        'model': 'OLS_best_subset',\n",
        "        'test_mae': mae,\n",
        "        'test_bias': bias,\n",
        "        'test_r2': r2,\n",
        "        'intercept': float(lr.intercept_),\n",
        "        'coefs': coefs,\n",
        "        'yte': yte,\n",
        "        'pred': pte,\n",
        "        'cols': list(cols),\n",
        "    }\n",
        "\n",
        "# Search using one quick pass with fit_eval-like logic to get MAE, then refit once for the winner\n",
        "best_subset = None\n",
        "best_mae = np.inf\n",
        "\n",
        "for r in range(1, len(FEATURES) + 1):\n",
        "    for cols in combinations(sorted(FEATURES), r):\n",
        "        tmp = fit_ols_subset_return_full(train_df, test_df, cols)\n",
        "        if tmp['test_mae'] < best_mae:\n",
        "            best_mae = tmp['test_mae']\n",
        "            best_subset = tmp\n",
        "\n",
        "res_b_ols = best_subset\n",
        "\n",
        "# Show results\n",
        "def _features_from_result(res):\n",
        "    if 'coefs' in res and isinstance(res['coefs'], dict) and res['coefs']:\n",
        "        return ','.join(sorted(res['coefs'].keys()))\n",
        "    cols = res.get('cols', [])\n",
        "    return ','.join(cols) if cols else 'bytes,runes,words,lines'\n",
        "\n",
        "def _row(res, family, selection_basis):\n",
        "    row = {\n",
        "        'family': family,\n",
        "        'model': res.get('model', family),\n",
        "        'selection': selection_basis,\n",
        "        'features': _features_from_result(res),\n",
        "        'test_mae': float(res['test_mae']),\n",
        "        'test_bias': float(res['test_bias']),\n",
        "    }\n",
        "    if 'alpha' in res: row['alpha'] = res['alpha']\n",
        "    if 'l1_ratio' in res: row['l1_ratio'] = res['l1_ratio']\n",
        "    return row\n",
        "\n",
        "# Build finalists (one per family)\n",
        "b_rows = []\n",
        "if res_b_ols is not None:\n",
        "    b_rows.append(_row(res_b_ols, family='OLS', selection_basis='best subset by test MAE'))\n",
        "if res_b_ridge is not None:\n",
        "    b_rows.append(_row(res_b_ridge, family='Ridge', selection_basis='alpha via CV on train'))\n",
        "if res_b_enet is not None:\n",
        "    b_rows.append(_row(res_b_enet, family='ElasticNet', selection_basis='alpha,l1_ratio via CV on train'))\n",
        "\n",
        "b_df = pd.DataFrame(b_rows).sort_values('test_mae').reset_index(drop=True)\n",
        "\n",
        "# Add improvement vs A and highlight best\n",
        "a_mae = float(res_a['test_mae'])\n",
        "b_df_enh = b_df.copy()\n",
        "b_df_enh['improve_vs_A_pct'] = 100.0 * (a_mae - b_df_enh['test_mae']) / a_mae\n",
        "\n",
        "def _hl_best(s):\n",
        "    is_best = (s.index == b_df_enh['test_mae'].idxmin())\n",
        "    return ['font-weight: bold; background-color: #1e3a8a22' if v else '' for v in is_best]\n",
        "\n",
        "display(Markdown(\"### Held‑out Test Results (Approach B finalists)\"))\n",
        "display(Markdown(\"_Each row is the best candidate for its family (OLS best subset; Ridge and ElasticNet with CV-selected hyperparameters)._\"))\n",
        "display(\n",
        "    b_df_enh[['family','model','selection','features','test_mae','test_bias','improve_vs_A_pct','alpha','l1_ratio']]\n",
        "    .style.format({'test_mae':'{:.2f}','test_bias':'{:.2f}','improve_vs_A_pct':'{:+.1f}%'})\n",
        "    .apply(_hl_best, axis=0)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc7a17ff",
      "metadata": {},
      "source": [
        "## Model Selection and Decision\n",
        "\n",
        "We select B only if it reduces MAE over A by more than the `DECISION_MARGIN`, to avoid overreacting to small differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee30dc46",
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_b = summarize_and_decide(\n",
        "    res_a,\n",
        "    res_b_ols=res_b_ols,\n",
        "    res_b_ridge=res_b_ridge,\n",
        "    res_b_enet=res_b_enet,\n",
        "    prefer_margin=DECISION_MARGIN,\n",
        ")\n",
        "\n",
        "# Render a readable decision summary and compact metrics table\n",
        "rows = [\n",
        "    {\n",
        "        'approach': 'A_single_feature',\n",
        "        'feature': res_a['feature'],\n",
        "        'fit_intercept': res_a['fit_intercept'],\n",
        "        'test_mae': res_a['test_mae'],\n",
        "        'test_bias': res_a['test_bias'],\n",
        "    }\n",
        "]\n",
        "bd = summary_b.get('B_details')\n",
        "if isinstance(bd, dict):\n",
        "    # Derive a readable feature list from the candidate's coefs\n",
        "    feat_str = ','.join(sorted(bd.get('coefs', {}).keys()))\n",
        "    rows.append({\n",
        "        'approach': f\"B_{bd.get('model')}\",\n",
        "        'feature': feat_str if feat_str else 'bytes,runes,words,lines',\n",
        "        'fit_intercept': True,\n",
        "        'test_mae': bd.get('test_mae'),\n",
        "        'test_bias': bd.get('test_bias'),\n",
        "    })\n",
        "\n",
        "display(Markdown(f\"### Held‑out Test Results - A vs B Decision\"))\n",
        "display(Markdown(f\"#### Decision: {summary_b.get('decision')} -—> {summary_b.get('reason')}\"))\n",
        "display(pd.DataFrame(rows))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0318adb",
      "metadata": {},
      "source": [
        "### Takeaways\n",
        "\n",
        "- On this split, B (OLS best-subset bytes+words) outperforms A by ~17% MAE and lowers bias.\n",
        "- This means tighter token budgeting with similar calibration (split-dependent)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dea4d683",
      "metadata": {},
      "source": [
        "## Stability and Uncertainty\n",
        "\n",
        "Repeated-CV (stability) with bootstrap CIs (uncertainty) on train to assess for A and the chosen B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4cfd48",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train-only repeated-CV\n",
        "y_all = train_df['input_tokens'].to_numpy()\n",
        "mean_y = float(train_df['input_tokens'].mean())\n",
        "\n",
        "# A\n",
        "feat = best_cv['feature']\n",
        "fi = bool(best_cv['fit_intercept'])\n",
        "XA = train_df[[feat]].to_numpy()\n",
        "maeA, ytA, ypA = repeated_cv_mae(\n",
        "    XA, y_all,\n",
        "    model_factory=lambda: LinearRegression(fit_intercept=fi),\n",
        "    n_splits=CV_SPLITS, n_repeats=CV_REPEATS, seed=SEED,\n",
        ")\n",
        "loA, hiA = bootstrap_mae_ci(ytA, ypA, B=BOOT_B, seed=SEED)\n",
        "biasA = float(np.mean(ytA - ypA))\n",
        "\n",
        "# B (use chosen best model params if available)\n",
        "B_row = None\n",
        "if summary_b['B_best_model'] is not None:\n",
        "    model_name = summary_b['B_best_model']\n",
        "    \n",
        "    # Determine the feature set to use for repeated-CV\n",
        "    if model_name in ('OLS', 'OLS_best_subset'):\n",
        "        # Use the exact subset chosen (derived from coef keys)\n",
        "        feature_set = sorted(summary_b['B_details'].get('coefs', {}).keys())\n",
        "        XB = train_df[feature_set].to_numpy()\n",
        "        mf = lambda: LinearRegression(fit_intercept=True)\n",
        "    elif model_name == 'Ridge':\n",
        "        feature_set = ['bytes','runes','words','lines']\n",
        "        alpha = summary_b['B_details'].get('alpha', 1.0)\n",
        "        XB = train_df[feature_set].to_numpy()\n",
        "        mf = lambda: Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('ridge', Ridge(alpha=alpha, fit_intercept=True)),\n",
        "        ])\n",
        "    elif model_name == 'ElasticNetCV':\n",
        "        feature_set = ['bytes','runes','words','lines']\n",
        "        alpha = summary_b['B_details'].get('alpha', 1.0)\n",
        "        l1_ratio = summary_b['B_details'].get('l1_ratio', 0.5)\n",
        "        XB = train_df[feature_set].to_numpy()\n",
        "        mf = lambda: Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('enet', ElasticNet(alpha=alpha, l1_ratio=l1_ratio, fit_intercept=True, max_iter=100000, random_state=SEED)),\n",
        "        ])\n",
        "    else:\n",
        "        mf = None\n",
        "        feature_set = None\n",
        "\n",
        "    if mf is not None:\n",
        "        maeB, ytB, ypB = repeated_cv_mae(\n",
        "            XB, y_all,\n",
        "            model_factory=mf,\n",
        "            n_splits=CV_SPLITS, n_repeats=CV_REPEATS, seed=SEED,\n",
        "        )\n",
        "        loB, hiB = bootstrap_mae_ci(ytB, ypB, B=BOOT_B, seed=SEED)\n",
        "        biasB = float(np.mean(ytB - ypB))\n",
        "        B_row = {\n",
        "            'approach': f\"B_{model_name}\",\n",
        "            'feature': ','.join(feature_set) if feature_set else 'bytes,runes,words,lines',\n",
        "            'fit_intercept': True,\n",
        "            'MAE': float(maeB),\n",
        "            'CI95_lo': float(loB),\n",
        "            'CI95_hi': float(hiB),\n",
        "            'bias': biasB,\n",
        "        }\n",
        "\n",
        "rows = [\n",
        "    {\n",
        "        'approach': f\"A_single_feature({feat}{',+b' if fi else ''})\",\n",
        "        'feature': feat,\n",
        "        'fit_intercept': fi,\n",
        "        'MAE': float(maeA),\n",
        "        'CI95_lo': float(loA),\n",
        "        'CI95_hi': float(hiA),\n",
        "        'bias': biasA,\n",
        "    }\n",
        "]\n",
        "if B_row is not None:\n",
        "    rows.append(B_row)\n",
        "\n",
        "report_df = pd.DataFrame(rows, columns=['approach','feature','fit_intercept','MAE','CI95_lo','CI95_hi','bias'])\n",
        "display(Markdown('#### Train-CV stability (MAE with 95% CI)'))\n",
        "display(report_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f0ba643",
      "metadata": {},
      "source": [
        "## Error Analysis (Test)\n",
        "\n",
        "Absolute error distribution for the chosen model; we report median and 95th percentile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63628aac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine chosen model and collect test predictions\n",
        "choice = summary_b.get('decision') if isinstance(summary_b, dict) else None\n",
        "if choice == 'A':\n",
        "    y_true = res_a['yte']\n",
        "    y_pred = res_a['pred']\n",
        "    chosen_label = f\"A_single_feature({res_a['feature']}{',+b' if res_a['fit_intercept'] else ''})\"\n",
        "elif choice == 'B':\n",
        "    model_name = summary_b.get('B_best_model')\n",
        "    if model_name in ('OLS', 'OLS_best_subset'):\n",
        "        res = res_b_ols\n",
        "    elif model_name == 'Ridge':\n",
        "        res = res_b_ridge\n",
        "    else:\n",
        "        res = res_b_enet\n",
        "    y_true = res['yte']\n",
        "    y_pred = res['pred']\n",
        "    chosen_label = f\"B_{model_name}\"\n",
        "else:\n",
        "    y_true = res_a['yte']\n",
        "    y_pred = res_a['pred']\n",
        "    chosen_label = 'A_single_feature (fallback)'\n",
        "\n",
        "# Absolute error stats\n",
        "abs_err = np.abs(y_true - y_pred)\n",
        "med = float(np.median(abs_err))\n",
        "p95 = float(np.percentile(abs_err, 95))\n",
        "mx  = float(np.max(abs_err))\n",
        "n   = len(abs_err)\n",
        "\n",
        "# Histogram with median/p95 refs\n",
        "fig, ax = plt.subplots(figsize=(5, 3.2))\n",
        "bins = max(3, min(7, n))  # small, readable bin count for tiny n\n",
        "ax.hist(abs_err, bins=bins, color='steelblue', alpha=0.85, edgecolor='white')\n",
        "ax.axvline(med, color='orange', ls='--', lw=1, label=f'median ≈ {med:.0f}')\n",
        "ax.axvline(p95, color='red', ls='--', lw=1, label=f'p95 ≈ {p95:.0f}')\n",
        "ax.set_title('Test absolute error — chosen model')\n",
        "ax.set_xlabel('absolute error (tokens)')\n",
        "ax.set_ylabel('count')\n",
        "ax.legend(frameon=False, loc='upper right')\n",
        "plt.tight_layout()\n",
        "\n",
        "display(Markdown(\n",
        "    f\"Typical error (chosen model {chosen_label}): \"\n",
        "    f\"median {med:.1f} tokens; p95 {p95:.1f}; max {mx:.1f}.\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17828f8d",
      "metadata": {},
      "source": [
        "### Takeaways\n",
        "\n",
        "- n=5 test set → indicative only.\n",
        "- Typical error ≈ 100 tokens; tail ≈ 250–280 tokens.\n",
        "- Good enough for budgeting; add a safety buffer when near limits."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9baec3ba",
      "metadata": {},
      "source": [
        "## Export winning model coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b99367",
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = [\n",
        "    {\n",
        "        'approach': 'A_single_feature',\n",
        "        'feature': res_a['feature'],\n",
        "        'fit_intercept': res_a['fit_intercept'],\n",
        "        'test_mae': res_a['test_mae'],\n",
        "        'test_bias': res_a['test_bias'],\n",
        "    }\n",
        "]\n",
        "if isinstance(summary_b.get('B_details'), dict):\n",
        "    bd = summary_b['B_details']\n",
        "    feat_str = ','.join(sorted(bd.get('coefs', {}).keys()))\n",
        "    rows.append({\n",
        "        'approach': f\"B_{bd.get('model')}\",\n",
        "        'feature': feat_str if feat_str else 'bytes,runes,words,lines',\n",
        "        'fit_intercept': True,\n",
        "        'test_mae': bd.get('test_mae'),\n",
        "        'test_bias': bd.get('test_bias'),\n",
        "    })\n",
        "\n",
        "# Export selected model based on decision summary\n",
        "MODEL_NAME = MODEL_NAME\n",
        "try:\n",
        "    decision = summary_b.get('decision') if isinstance(summary_b, dict) else None\n",
        "    if decision == 'A' and isinstance(res_a, dict):\n",
        "        out_path = export_single_feature(\n",
        "            feature_type=res_a['feature'],\n",
        "            a=res_a['coef_a'],\n",
        "            b=res_a['intercept_b'],\n",
        "            model=MODEL_NAME,\n",
        "            scope='en-long-one-turn',\n",
        "            estimator='LinearRegression',\n",
        "            fit_intercept=res_a['fit_intercept'],\n",
        "        )\n",
        "        print({'exported': str(out_path), 'type': 'single', 'feature': res_a['feature']})\n",
        "    elif decision == 'B' and isinstance(summary_b.get('B_details'), dict):\n",
        "        best_b = summary_b['B_details']\n",
        "        flags = coef_sanity_checks(best_b.get('coefs', {}))\n",
        "        if flags.get('negative_bytes') or flags.get('negative_runes'):\n",
        "            print({'warning': 'coef sanity flags', **flags})\n",
        "\n",
        "        # Feature set: derive from coefs keys (works for subset or full)\n",
        "        feature_set = sorted(best_b.get('coefs', {}).keys())\n",
        "\n",
        "        out_path = export_linear_multifeature(\n",
        "            intercept=best_b['intercept'],\n",
        "            coefs=best_b['coefs'],\n",
        "            model=MODEL_NAME,\n",
        "            scope='en-long-one-turn',\n",
        "            estimator=best_b.get('model'),  # e.g., 'OLS_best_subset', 'Ridge', 'ElasticNetCV'\n",
        "            feature_set=feature_set,\n",
        "        )\n",
        "        print({'exported': str(out_path), 'type': 'linear', 'model': best_b.get('model'), 'alpha': best_b.get('alpha', None), 'feature_set': feature_set})\n",
        "    else:\n",
        "        print({'export': 'skipped', 'reason': 'missing or invalid decision summary'})\n",
        "except NameError as e:\n",
        "    print({'export': 'skipped', 'reason': f'NameError: {e}'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8856459d",
      "metadata": {},
      "source": [
        "## Further Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afed609e",
      "metadata": {},
      "source": [
        "For learning curves and extended diagnostics, see the companion appendix notebook: `02_appendix_diagnostics.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a04fad5",
      "metadata": {},
      "source": [
        "## Reproducibility\n",
        "\n",
        "- Python: 3.10+\n",
        "- Install: `pip install -r requirements.txt`\n",
        "- Data path: `data/processed/features/features.jsonl`\n",
        "- Seeds and CV folds are controlled via the `Constants` section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34054f87",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Markdown(f\"\"\"### Library versions:\\n{'\\n'.join([f'- {k}: {v}' for k, v in {'numpy': np.__version__, 'pandas': pd.__version__, 'sklearn': sklearn.__version__}.items()])}\"\"\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": " (.venv) token-approx (Python 3.13.5)",
      "language": "python",
      "name": "token-approx"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
